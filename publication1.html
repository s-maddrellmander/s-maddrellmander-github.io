<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MiniMol: A Parameter-Efficient Foundation Model - Sam Maddrell-Mander</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.8;
            color: #333;
            background-color: #fafafa;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 20px 0;
        }
        h1, h2 {
            color: #0071e3;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 20px 0;
        }
        .cta-button {
            background-color: #0071e3;
            color: #fff;
            padding: 12px 24px;
            border-radius: 30px;
            text-decoration: none;
            font-weight: 500;
            font-size: 14px;
            transition: background-color 0.3s ease, transform 0.3s ease;
            display: inline-block;
        }
        .cta-button:hover {
            background-color: #0077ed;
            transform: translateY(-2px);
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>MiniMol: A Parameter-Efficient Foundation Model for Molecular Learning</h1>
        </div>
    </header>
    <main>
        <div class="container">
            <p><strong>ICML Workshop Spotlight Paper, 2024</strong></p>
            <p>This paper introduces MiniMol, a parameter-efficient foundation model for molecular learning. We demonstrate that by leveraging advanced pre-training techniques and a novel architecture, MiniMol achieves comparable performance to larger models while using significantly fewer parameters.</p>
            <h2>Key Contributions</h2>
            <ul>
                <li>Novel architecture combining aspects of transformers and graph neural networks</li>
                <li>Efficient pre-training strategy for molecular representations</li>
                <li>Comprehensive evaluation on multiple molecular property prediction tasks</li>
                <li>Analysis of model interpretability and its implications for drug discovery</li>
            </ul>
            <a href="index.html#publications" class="cta-button">Back to Publications</a>
        </div>
    </main>
</body>
</html>
