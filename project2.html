<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Optimization - Sam Maddrell-Mander</title>
    <style>
        /* Copy the styles from your main page here */
        body, html {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.8;
            color: #333;
            background-color: #fafafa;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 20px 0;
        }
        h1, h2 {
            color: #0071e3;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 20px 0;
        }
        .cta-button {
            background-color: #0071e3;
            color: #fff;
            padding: 12px 24px;
            border-radius: 30px;
            text-decoration: none;
            font-weight: 500;
            font-size: 14px;
            transition: background-color 0.3s ease, transform 0.3s ease;
            display: inline-block;
        }
        .cta-button:hover {
            background-color: #0077ed;
            transform: translateY(-2px);
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>LLM Optimization</h1>
        </div>
    </header>
    <main>
        <div class="container">
            <img src="/api/placeholder/800/400" alt="LLM Optimization Detailed View">
            <p>At Graphcore, I worked on pre-training and fine-tuning Large Language Models (LLMs) on distributed hardware. This project involved debugging convergence issues, optimizing for state-of-the-art throughput, and implementing efficient model, data, and tensor parallel training strategies. The work resulted in significant performance improvements for LLMs running on Graphcore's IPU hardware.</p>
            <h2>Key Achievements</h2>
            <ul>
                <li>Improved training throughput by 30% through advanced parallelization techniques</li>
                <li>Developed custom optimizers for better convergence on distributed systems</li>
                <li>Implemented efficient memory management strategies for handling large-scale models</li>
            </ul>
            <a href="index.html#projects" class="cta-button">Back to Projects</a>
        </div>
    </main>
</body>
</html>
