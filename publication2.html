<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Foundational Models for Molecular Learning - Sam Maddrell-Mander</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.8;
            color: #333;
            background-color: #fafafa;
        }
        .container {
            max-width: 800px;
            margin: 0 auto;
            padding: 0 20px;
        }
        header {
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            padding: 20px 0;
        }
        h1, h2 {
            color: #0071e3;
        }
        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 20px 0;
        }
        .cta-button {
            background-color: #0071e3;
            color: #fff;
            padding: 12px 24px;
            border-radius: 30px;
            text-decoration: none;
            font-weight: 500;
            font-size: 14px;
            transition: background-color 0.3s ease, transform 0.3s ease;
            display: inline-block;
        }
        .cta-button:hover {
            background-color: #0077ed;
            transform: translateY(-2px);
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Towards Foundational Models for Molecular Learning on Large Scale Multi-Task Datasets</h1>
        </div>
    </header>
    <main>
        <div class="container">
            <p><strong>ICLR 2024</strong></p>
            <p>This paper explores the development of foundational models for molecular learning using large-scale multi-task datasets. We present a novel approach to pre-training and fine-tuning that enables efficient transfer learning across a wide range of molecular property prediction tasks.</p>
            <h2>Key Contributions</h2>
            <ul>
                <li>Creation of a large-scale, multi-task molecular dataset for pre-training</li>
                <li>Development of a new pre-training objective tailored for molecular representations</li>
                <li>Extensive evaluation on downstream tasks, demonstrating strong transfer learning capabilities</li>
                <li>Analysis of the model's ability to capture meaningful chemical patterns and properties</li>
            </ul>
            <a href="index.html#publications" class="cta-button">Back to Publications</a>
        </div>
    </main>
</body>
</html>
