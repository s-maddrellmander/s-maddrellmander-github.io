<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MiniMol: A Parameter-Efficient Foundation Model - Sam Maddrell-Mander</title>
    <style>
        body, html {
            margin: 0;
            padding: 0;
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f5f5f7;
        }
        .container {
            max-width: 1000px;
            margin: 0 auto;
            padding: 40px 20px;
        }
        header {
            background-color: rgba(255, 255, 255, 0.8);
            backdrop-filter: blur(10px);
            position: sticky;
            top: 0;
            z-index: 1000;
        }
        h1 {
            font-size: 40px;
            font-weight: 700;
            color: #1d1d1f;
            margin-bottom: 10px;
        }
        h2 {
            font-size: 28px;
            font-weight: 600;
            color: #1d1d1f;
            margin-top: 40px;
        }
        p, li {
            font-size: 18px;
            color: #515154;
        }
        strong {
            color: #1d1d1f;
        }
        .paper-preview {
            display: flex;
            align-items: center;
            background-color: white;
            border-radius: 18px;
            overflow: hidden;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
            margin: 40px 0;
        }
        .paper-preview img {
            width: 40%;
            object-fit: cover;
            height: 300px;
        }
        .paper-preview-content {
            padding: 30px;
        }
        .cta-button {
            background-color: #000;
            color: #fff;
            padding: 12px 24px;
            border-radius: 30px;
            text-decoration: none;
            font-weight: 500;
            font-size: 16px;
            transition: background-color 0.3s ease, transform 0.3s ease;
            display: inline-block;
            margin-top: 20px;
        }
        .cta-button:hover {
            background-color: #333;
            transform: translateY(-2px);
        }
        .paper-link {
            display: inline-block;
            margin-top: 20px;
            color: #06c;
            text-decoration: none;
            font-weight: 500;
        }
        .paper-link:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>Towards Foundational Models for Molecular Learning on Large-Scale Multi-Task Datasets</h1>
            <p><strong>ICLR Poster, 2024</strong></p>
        </div>
    </header>
    <main>
        <div class="container">
            <div class="paper-preview">
                <img src="images/graphium.png" alt="First page of MiniMol paper">
                <div class="paper-preview-content">
                    <p>This paper introduces seven novel datasets and the Graphium library for advancing molecular machine learning.</p>
                    <a href="https://openreview.net/forum?id=Zc2aIcucwc" class="paper-link" target="_blank">Read the full paper on arXiv</a>
                </div>
            </div>
            
            <h2>Key Contributions</h2>
            <ul>
                <li>Presented seven novel datasets for molecular machine learning, categorized as ToyMix, LargeMix, and UltraLarge.</li>
                <li>The datasets cover nearly 100 million molecules and over 3000 sparsely defined tasks, totaling more than 13 billion individual labels of both quantum and biological nature.</li>
                <li>Compared to existing datasets, ours contain 300 times more data points than the OGB-LSC PCQM4Mv2 dataset, and 13 times more than the quantum-only QM1B dataset.</li>
                <li>Introduced the Graphium graph machine learning library to simplify the process of building and training molecular machine learning models for multi-task and multi-level molecular datasets.</li>
                <li>Provided baseline results for multi-task and multi-level training on these datasets.</li>
                <li>Observed that performance on low-resource biological datasets improves by also training on large amounts of quantum data, indicating potential in multi-task and multi-level training of a foundation model and fine-tuning it to resource-constrained downstream tasks.</li>
                <li>Made the Graphium library publicly available on Github and provided dataset links in Part 1 and Part 2.</li>
            </ul>
            
            <a href="index.html#publications" class="cta-button">Back to Publications</a>
        </div>
    </main>
</body>
</html>
